{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b566e84e-4bef-43e7-96e9-a357f47713cc",
   "metadata": {},
   "source": [
    "\n",
    "Il est conçu avec PyTorch Lightning, une surcouche de PyTorch qui simplifie l'entraînement.\n",
    "PyTorch Lightning est une bibliothèque qui repose sur PyTorch, mais qui simplifie et organise le processus de développement des modèles de machine learning (ML). Son objectif est de réduire le code boilerplate et de faciliter l'entraînement de modèles complexes tout en gardant la flexibilité de PyTorch.\n",
    "\n",
    "PyTorch Lightning sépare clairement les responsabilités du modèle et de l'entraînement. Avec Lightning, ton modèle est défini dans une classe (souvent appelée LightningModule), et il y a des méthodes dédiées pour chaque partie du processus d'entraînement, de validation et de test.\n",
    "\n",
    "En PyTorch standard, tu dois écrire tout le code pour l'entraînement, la validation et la gestion des optimizers, ce qui peut rapidement devenir compliqué et répétitif.\n",
    "En PyTorch Lightning, tout cela est simplifié et géré de manière automatique.\n",
    "\n",
    "PyTorch Lightning gère tout ce qui est récurrent dans l'entraînement des modèles ML, comme :\n",
    "\n",
    "Gestion des optimizers et des schedulers : Définis-les une seule fois, et PyTorch Lightning gère leur utilisation pendant l'entraînement.\n",
    "Gestion des \"checkpoints\" : Sauvegarde automatique du modèle pendant l'entraînement, ce qui te permet de reprendre l'entraînement à partir du dernier état enregistré.\n",
    "Répartition des tâches sur plusieurs GPUs / TPUs : Si tu utilises plusieurs GPUs ou même des TPUs, Lightning le gère pour toi avec peu de code supplémentaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14ad6b-bc51-4725-a001-8023cd702e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf2cdc6-d467-461e-bc0d-83ae8fce3d37",
   "metadata": {},
   "source": [
    "Import des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91651fd-78c4-4e54-8df0-df69644df955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646277c-484f-40ad-9be5-986f54a10689",
   "metadata": {},
   "source": [
    "## Définition de la classe correspondnat au modèle : contient\n",
    "- La définition des couches du modéle, et fonction pour le modèle (`__init__`)\n",
    "- Le workflow d'execution du modèle `forward`\n",
    "- `configure_optimizers`\n",
    "- Le workflow d'entrainement du modèle `trainnig_step` \n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63658cba-4dcc-47ad-8c56-50c3ebe932f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MonModele(pl.LightningModule):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Première couche linéaire\n",
    "        self.relu = nn.ReLU()  # Activation ReLU\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Deuxième couche linéaire\n",
    "        self.learning_rate = learning_rate  # Taux d'apprentissage\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Passage dans la couche 1\n",
    "        x = self.relu(x)  # Activation ReLU\n",
    "        x = self.fc2(x)  # Passage dans la couche 2\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)  # Prédiction du modèle\n",
    "        loss = nn.functional.mse_loss(y_hat, y)  # Calcul de l'erreur (MSE)\n",
    "        self.log(\"train_loss\", loss)  # Sauvegarde de la perte\n",
    "        return loss\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2703c-122c-4961-9f1c-2175c7ede115",
   "metadata": {},
   "source": [
    "# Utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151ac63-fa3a-4b01-bc30-bdeb86c6703e",
   "metadata": {},
   "source": [
    "## entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9bcef-3e70-4d65-a059-8c76271cba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dataset simple pour la régression avec 2 features\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n",
    "\n",
    "# Convertir en tensor PyTorch\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Reshape pour correspondre à la sortie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9301e-5276-4bac-8b4a-396601caf12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5)\n",
    "trainer.fit(model, train_loader)  # Entraînement avec un DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
